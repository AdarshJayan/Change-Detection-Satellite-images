{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNrWVSoWc-dZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d6c94d-0a55-42f9-9e4d-cde72d2a854b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                       Version\n",
            "----------------------------- --------------------\n",
            "absl-py                       1.4.0\n",
            "alabaster                     0.7.13\n",
            "albumentations                1.2.1\n",
            "altair                        4.2.2\n",
            "anyio                         3.6.2\n",
            "appdirs                       1.4.4\n",
            "argon2-cffi                   21.3.0\n",
            "argon2-cffi-bindings          21.2.0\n",
            "array-record                  0.2.0\n",
            "arviz                         0.15.1\n",
            "astropy                       5.2.2\n",
            "astunparse                    1.6.3\n",
            "attrs                         23.1.0\n",
            "audioread                     3.0.0\n",
            "autograd                      1.5\n",
            "Babel                         2.12.1\n",
            "backcall                      0.2.0\n",
            "beautifulsoup4                4.11.2\n",
            "bleach                        6.0.0\n",
            "blis                          0.7.9\n",
            "blosc2                        2.0.0\n",
            "bokeh                         2.4.3\n",
            "branca                        0.6.0\n",
            "build                         0.10.0\n",
            "CacheControl                  0.12.11\n",
            "cached-property               1.5.2\n",
            "cachetools                    5.3.0\n",
            "catalogue                     2.0.8\n",
            "certifi                       2022.12.7\n",
            "cffi                          1.15.1\n",
            "chardet                       4.0.0\n",
            "charset-normalizer            2.0.12\n",
            "chex                          0.1.7\n",
            "click                         8.1.3\n",
            "cloudpickle                   2.2.1\n",
            "cmake                         3.25.2\n",
            "cmdstanpy                     1.1.0\n",
            "colorcet                      3.0.1\n",
            "colorlover                    0.3.0\n",
            "community                     1.0.0b1\n",
            "confection                    0.0.4\n",
            "cons                          0.4.5\n",
            "contextlib2                   0.6.0.post1\n",
            "contourpy                     1.0.7\n",
            "convertdate                   2.4.0\n",
            "cryptography                  40.0.2\n",
            "cufflinks                     0.17.3\n",
            "cupy-cuda11x                  11.0.0\n",
            "cvxopt                        1.3.0\n",
            "cvxpy                         1.3.1\n",
            "cycler                        0.11.0\n",
            "cymem                         2.0.7\n",
            "Cython                        0.29.34\n",
            "dask                          2022.12.1\n",
            "datascience                   0.17.6\n",
            "db-dtypes                     1.1.1\n",
            "dbus-python                   1.2.16\n",
            "debugpy                       1.6.6\n",
            "decorator                     4.4.2\n",
            "defusedxml                    0.7.1\n",
            "distributed                   2022.12.1\n",
            "dlib                          19.24.1\n",
            "dm-tree                       0.1.8\n",
            "docutils                      0.16\n",
            "dopamine-rl                   4.0.6\n",
            "duckdb                        0.7.1\n",
            "earthengine-api               0.1.350\n",
            "easydict                      1.10\n",
            "ecos                          2.0.12\n",
            "editdistance                  0.6.2\n",
            "en-core-web-sm                3.5.0\n",
            "entrypoints                   0.4\n",
            "ephem                         4.1.4\n",
            "et-xmlfile                    1.1.0\n",
            "etils                         1.2.0\n",
            "etuples                       0.3.8\n",
            "exceptiongroup                1.1.1\n",
            "fastai                        2.7.12\n",
            "fastcore                      1.5.29\n",
            "fastdownload                  0.0.7\n",
            "fastjsonschema                2.16.3\n",
            "fastprogress                  1.0.3\n",
            "fastrlock                     0.8.1\n",
            "filelock                      3.12.0\n",
            "firebase-admin                5.3.0\n",
            "Flask                         2.2.4\n",
            "flatbuffers                   23.3.3\n",
            "flax                          0.6.9\n",
            "folium                        0.14.0\n",
            "fonttools                     4.39.3\n",
            "frozendict                    2.3.7\n",
            "fsspec                        2023.4.0\n",
            "future                        0.18.3\n",
            "gast                          0.4.0\n",
            "GDAL                          3.3.2\n",
            "gdown                         4.6.6\n",
            "gensim                        4.3.1\n",
            "geographiclib                 2.0\n",
            "geopy                         2.3.0\n",
            "gin-config                    0.5.0\n",
            "glob2                         0.7\n",
            "google                        2.0.3\n",
            "google-api-core               2.11.0\n",
            "google-api-python-client      2.84.0\n",
            "google-auth                   2.17.3\n",
            "google-auth-httplib2          0.1.0\n",
            "google-auth-oauthlib          1.0.0\n",
            "google-cloud-bigquery         3.9.0\n",
            "google-cloud-bigquery-storage 2.19.1\n",
            "google-cloud-core             2.3.2\n",
            "google-cloud-datastore        2.15.1\n",
            "google-cloud-firestore        2.11.0\n",
            "google-cloud-language         2.9.1\n",
            "google-cloud-storage          2.8.0\n",
            "google-cloud-translate        3.11.1\n",
            "google-colab                  1.0.0\n",
            "google-crc32c                 1.5.0\n",
            "google-pasta                  0.2.0\n",
            "google-resumable-media        2.5.0\n",
            "googleapis-common-protos      1.59.0\n",
            "googledrivedownloader         0.4\n",
            "graphviz                      0.20.1\n",
            "greenlet                      2.0.2\n",
            "grpcio                        1.54.0\n",
            "grpcio-status                 1.48.2\n",
            "gspread                       3.4.2\n",
            "gspread-dataframe             3.0.8\n",
            "gym                           0.25.2\n",
            "gym-notices                   0.0.8\n",
            "h5netcdf                      1.1.0\n",
            "h5py                          3.8.0\n",
            "hijri-converter               2.3.1\n",
            "holidays                      0.23\n",
            "holoviews                     1.15.4\n",
            "html5lib                      1.1\n",
            "httpimport                    1.3.0\n",
            "httplib2                      0.21.0\n",
            "humanize                      4.6.0\n",
            "hyperopt                      0.2.7\n",
            "idna                          3.4\n",
            "imageio                       2.25.1\n",
            "imageio-ffmpeg                0.4.8\n",
            "imagesize                     1.4.1\n",
            "imbalanced-learn              0.10.1\n",
            "imgaug                        0.4.0\n",
            "importlib-resources           5.12.0\n",
            "imutils                       0.5.4\n",
            "inflect                       6.0.4\n",
            "iniconfig                     2.0.0\n",
            "intel-openmp                  2023.1.0\n",
            "ipykernel                     5.5.6\n",
            "ipython                       7.34.0\n",
            "ipython-genutils              0.2.0\n",
            "ipython-sql                   0.4.1\n",
            "ipywidgets                    7.7.1\n",
            "itsdangerous                  2.1.2\n",
            "jax                           0.4.8\n",
            "jaxlib                        0.4.7+cuda11.cudnn86\n",
            "jieba                         0.42.1\n",
            "Jinja2                        3.1.2\n",
            "joblib                        1.2.0\n",
            "jsonpickle                    3.0.1\n",
            "jsonschema                    4.3.3\n",
            "jupyter-client                6.1.12\n",
            "jupyter-console               6.1.0\n",
            "jupyter_core                  5.3.0\n",
            "jupyter-server                1.24.0\n",
            "jupyterlab-pygments           0.2.2\n",
            "jupyterlab-widgets            3.0.7\n",
            "kaggle                        1.5.13\n",
            "keras                         2.12.0\n",
            "kiwisolver                    1.4.4\n",
            "korean-lunar-calendar         0.3.1\n",
            "langcodes                     3.3.0\n",
            "lazy_loader                   0.2\n",
            "libclang                      16.0.0\n",
            "librosa                       0.10.0.post2\n",
            "lightgbm                      3.3.5\n",
            "lit                           16.0.5\n",
            "llvmlite                      0.39.1\n",
            "locket                        1.0.0\n",
            "logical-unification           0.4.5\n",
            "LunarCalendar                 0.0.9\n",
            "lxml                          4.9.2\n",
            "Markdown                      3.4.3\n",
            "markdown-it-py                2.2.0\n",
            "MarkupSafe                    2.1.2\n",
            "matplotlib                    3.7.1\n",
            "matplotlib-inline             0.1.6\n",
            "matplotlib-venn               0.11.9\n",
            "mdurl                         0.1.2\n",
            "miniKanren                    1.0.3\n",
            "missingno                     0.5.2\n",
            "mistune                       0.8.4\n",
            "mizani                        0.8.1\n",
            "mkl                           2019.0\n",
            "ml-dtypes                     0.1.0\n",
            "mlxtend                       0.14.0\n",
            "more-itertools                9.1.0\n",
            "moviepy                       1.0.3\n",
            "mpmath                        1.3.0\n",
            "msgpack                       1.0.5\n",
            "multipledispatch              0.6.0\n",
            "multitasking                  0.0.11\n",
            "murmurhash                    1.0.9\n",
            "music21                       8.1.0\n",
            "natsort                       8.3.1\n",
            "nbclient                      0.7.4\n",
            "nbconvert                     6.5.4\n",
            "nbformat                      5.8.0\n",
            "nest-asyncio                  1.5.6\n",
            "networkx                      3.1\n",
            "nibabel                       3.0.2\n",
            "nltk                          3.8.1\n",
            "notebook                      6.4.8\n",
            "numba                         0.56.4\n",
            "numexpr                       2.8.4\n",
            "numpy                         1.22.4\n",
            "oauth2client                  4.1.3\n",
            "oauthlib                      3.2.2\n",
            "opencv-contrib-python         4.7.0.72\n",
            "opencv-python                 4.7.0.72\n",
            "opencv-python-headless        4.7.0.72\n",
            "openpyxl                      3.0.10\n",
            "opt-einsum                    3.3.0\n",
            "optax                         0.1.5\n",
            "orbax-checkpoint              0.2.1\n",
            "osqp                          0.6.2.post8\n",
            "packaging                     23.1\n",
            "palettable                    3.3.3\n",
            "pandas                        1.5.3\n",
            "pandas-datareader             0.10.0\n",
            "pandas-gbq                    0.17.9\n",
            "pandocfilters                 1.5.0\n",
            "panel                         0.14.4\n",
            "param                         1.13.0\n",
            "parso                         0.8.3\n",
            "partd                         1.4.0\n",
            "pathlib                       1.0.1\n",
            "pathy                         0.10.1\n",
            "patsy                         0.5.3\n",
            "pexpect                       4.8.0\n",
            "pickleshare                   0.7.5\n",
            "Pillow                        8.4.0\n",
            "pip                           23.1.2\n",
            "pip-tools                     6.13.0\n",
            "platformdirs                  3.3.0\n",
            "plotly                        5.13.1\n",
            "plotnine                      0.10.1\n",
            "pluggy                        1.0.0\n",
            "polars                        0.17.3\n",
            "pooch                         1.6.0\n",
            "portpicker                    1.3.9\n",
            "prefetch-generator            1.0.3\n",
            "preshed                       3.0.8\n",
            "prettytable                   0.7.2\n",
            "proglog                       0.1.10\n",
            "progressbar2                  4.2.0\n",
            "prometheus-client             0.16.0\n",
            "promise                       2.3\n",
            "prompt-toolkit                3.0.38\n",
            "prophet                       1.1.2\n",
            "proto-plus                    1.22.2\n",
            "protobuf                      3.20.3\n",
            "psutil                        5.9.5\n",
            "psycopg2                      2.9.6\n",
            "ptyprocess                    0.7.0\n",
            "py-cpuinfo                    9.0.0\n",
            "py4j                          0.10.9.7\n",
            "pyarrow                       9.0.0\n",
            "pyasn1                        0.5.0\n",
            "pyasn1-modules                0.3.0\n",
            "pycocotools                   2.0.6\n",
            "pycparser                     2.21\n",
            "pyct                          0.5.0\n",
            "pydantic                      1.10.7\n",
            "pydata-google-auth            1.7.0\n",
            "pydot                         1.4.2\n",
            "pydot-ng                      2.0.0\n",
            "pydotplus                     2.0.2\n",
            "PyDrive                       1.3.1\n",
            "pyerfa                        2.0.0.3\n",
            "pygame                        2.3.0\n",
            "Pygments                      2.14.0\n",
            "PyGObject                     3.36.0\n",
            "pymc                          5.1.2\n",
            "PyMeeus                       0.5.12\n",
            "pymystem3                     0.2.0\n",
            "PyOpenGL                      3.1.6\n",
            "pyparsing                     3.0.9\n",
            "pyproject_hooks               1.0.0\n",
            "pyrsistent                    0.19.3\n",
            "PySocks                       1.7.1\n",
            "pytensor                      2.10.1\n",
            "pytest                        7.2.2\n",
            "python-apt                    0.0.0\n",
            "python-dateutil               2.8.2\n",
            "python-louvain                0.16\n",
            "python-slugify                8.0.1\n",
            "python-utils                  3.5.2\n",
            "pytz                          2022.7.1\n",
            "pytz-deprecation-shim         0.1.0.post0\n",
            "pyviz-comms                   2.2.1\n",
            "PyWavelets                    1.4.1\n",
            "PyYAML                        6.0\n",
            "pyzmq                         23.2.1\n",
            "qdldl                         0.1.7\n",
            "qudida                        0.0.4\n",
            "regex                         2022.10.31\n",
            "requests                      2.27.1\n",
            "requests-oauthlib             1.3.1\n",
            "requests-unixsocket           0.2.0\n",
            "requirements-parser           0.5.0\n",
            "rich                          13.3.4\n",
            "rpy2                          3.5.5\n",
            "rsa                           4.9\n",
            "scikit-image                  0.19.3\n",
            "scikit-learn                  1.2.2\n",
            "scipy                         1.10.1\n",
            "scs                           3.2.3\n",
            "seaborn                       0.12.2\n",
            "Send2Trash                    1.8.0\n",
            "setuptools                    67.7.2\n",
            "shapely                       2.0.1\n",
            "six                           1.16.0\n",
            "sklearn-pandas                2.2.0\n",
            "smart-open                    6.3.0\n",
            "sniffio                       1.3.0\n",
            "snowballstemmer               2.2.0\n",
            "sortedcontainers              2.4.0\n",
            "soundfile                     0.12.1\n",
            "soupsieve                     2.4.1\n",
            "soxr                          0.3.5\n",
            "spacy                         3.5.2\n",
            "spacy-legacy                  3.0.12\n",
            "spacy-loggers                 1.0.4\n",
            "Sphinx                        3.5.4\n",
            "sphinxcontrib-applehelp       1.0.4\n",
            "sphinxcontrib-devhelp         1.0.2\n",
            "sphinxcontrib-htmlhelp        2.0.1\n",
            "sphinxcontrib-jsmath          1.0.1\n",
            "sphinxcontrib-qthelp          1.0.3\n",
            "sphinxcontrib-serializinghtml 1.1.5\n",
            "SQLAlchemy                    2.0.10\n",
            "sqlparse                      0.4.4\n",
            "srsly                         2.4.6\n",
            "statsmodels                   0.13.5\n",
            "sympy                         1.11.1\n",
            "tables                        3.8.0\n",
            "tabulate                      0.8.10\n",
            "tblib                         1.7.0\n",
            "tenacity                      8.2.2\n",
            "tensorboard                   2.12.2\n",
            "tensorboard-data-server       0.7.0\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.12.0\n",
            "tensorflow-datasets           4.9.2\n",
            "tensorflow-estimator          2.12.0\n",
            "tensorflow-gcs-config         2.12.0\n",
            "tensorflow-hub                0.13.0\n",
            "tensorflow-io-gcs-filesystem  0.32.0\n",
            "tensorflow-metadata           1.13.1\n",
            "tensorflow-probability        0.20.0\n",
            "tensorstore                   0.1.36\n",
            "termcolor                     2.3.0\n",
            "terminado                     0.17.1\n",
            "text-unidecode                1.3\n",
            "textblob                      0.17.1\n",
            "tf-slim                       1.1.0\n",
            "thinc                         8.1.9\n",
            "threadpoolctl                 3.1.0\n",
            "tifffile                      2023.4.12\n",
            "tinycss2                      1.2.1\n",
            "toml                          0.10.2\n",
            "tomli                         2.0.1\n",
            "toolz                         0.12.0\n",
            "torch                         2.0.1+cu118\n",
            "torchaudio                    2.0.2+cu118\n",
            "torchdata                     0.6.1\n",
            "torchsummary                  1.5.1\n",
            "torchtext                     0.15.2\n",
            "torchvision                   0.15.2+cu118\n",
            "tornado                       6.3.1\n",
            "tqdm                          4.65.0\n",
            "traitlets                     5.7.1\n",
            "triton                        2.0.0\n",
            "tweepy                        4.13.0\n",
            "typer                         0.7.0\n",
            "types-setuptools              67.7.0.2\n",
            "typing_extensions             4.5.0\n",
            "tzdata                        2023.3\n",
            "tzlocal                       4.3\n",
            "uritemplate                   4.1.1\n",
            "urllib3                       1.26.15\n",
            "vega-datasets                 0.9.0\n",
            "wasabi                        1.1.1\n",
            "wcwidth                       0.2.6\n",
            "webcolors                     1.13\n",
            "webencodings                  0.5.1\n",
            "websocket-client              1.5.1\n",
            "Werkzeug                      2.3.0\n",
            "wheel                         0.40.0\n",
            "widgetsnbextension            3.6.4\n",
            "wordcloud                     1.8.2.2\n",
            "wrapt                         1.14.1\n",
            "xarray                        2022.12.0\n",
            "xarray-einstats               0.5.1\n",
            "xgboost                       1.7.5\n",
            "xlrd                          2.0.1\n",
            "yellowbrick                   1.5\n",
            "yfinance                      0.2.18\n",
            "zict                          3.0.0\n",
            "zipp                          3.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MANQ7fxrUTtG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torchgeo==0.4.0\n",
        "!pip install pygments==2.6.1\n",
        "!pip install pytorch-lightning==1.9.2\n",
        "!pip install torch==1.13.1\n",
        "!pip install torchvision==0.14.1\n",
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2JjQmPQUQ01"
      },
      "outputs": [],
      "source": [
        "import torchgeo\n",
        "from torchgeo.datasets import LEVIRCDPlus\n",
        "from torchgeo.datasets.utils import unbind_samples\n",
        "from torchgeo.trainers import SemanticSegmentationTask\n",
        "from torchgeo.datamodules.utils import dataset_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otOuhqQTUQ02"
      },
      "outputs": [],
      "source": [
        "#import torchvision\n",
        "from torchvision.transforms import Compose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "Q5YmqS3upwuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwBSOD89UQ03"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#from collections import OrderedDict\n",
        "\n",
        "\n",
        "import kornia.augmentation as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "5nunwIxzRMgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print (num_gpus)"
      ],
      "metadata": {
        "id": "A_avZATly_EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBviNxxHUQ03"
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ttzVhWaUQ04"
      },
      "outputs": [],
      "source": [
        "exp_name = \"exp_1\"\n",
        "exp_dir = f\"Checkpoint/{exp_name}\"\n",
        "os.makedirs(exp_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQgrosbAUQ04"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "lr = 0.01  #learning rate\n",
        "gpu_id = 0\n",
        "# device = torch.device(f\"cuda:{gpu_id}\")\n",
        "num_workers = 10\n",
        "patch_size = 256\n",
        "val_split_pct = 0.2\n",
        "\n",
        "train_dataset = LEVIRCDPlus(root=\"LEVIRCDPlus\", split=\"train\", download=True, checksum=True)\n",
        "test_dataset = LEVIRCDPlus(root=\"LEVIRCDPlus\", split=\"test\", download=True, checksum=True)\n",
        "\n",
        "print(f'train: {len(train_dataset)} images')\n",
        "print(f'test: {len(test_dataset)} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfgfQ9N3LGFk"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/LEVIRCDPlus/LEVIR-CD+/train'\n",
        "test_path = '/content/LEVIRCDPlus/LEVIR-CD+/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S6tK-M8LID1"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    samplewise_center=True,\n",
        "    samplewise_std_normalization=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hozAXsBLLMqm"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1 / 255.0,\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.05,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        shear_range=0.05,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "        validation_split=0.20)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1 / 255.0)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_path,\n",
        "    target_size=(180,180),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "valid_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_path,\n",
        "    target_size=(180,180),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    subset='validation',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=test_path,\n",
        "    target_size=(180,180),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=1,\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkqWLqj3UQ05"
      },
      "outputs": [],
      "source": [
        "class CustomSemanticSegmentationTask(SemanticSegmentationTask):\n",
        "\n",
        "    def plot(self, sample):\n",
        "\n",
        "        image1 = sample[\"image\"][:3]\n",
        "        image2 = sample[\"image\"][3:]\n",
        "        mask = sample[\"mask\"]\n",
        "        prediction = sample[\"prediction\"]\n",
        "\n",
        "\n",
        "        fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(4*5, 5))\n",
        "        axs[0].imshow(image1.permute(1,2,0)) #(3, 256, 256) --> (256, 256, 3)\n",
        "        axs[0].axis(\"off\")\n",
        "        axs[1].imshow(image2.permute(1,2,0))\n",
        "        axs[1].axis(\"off\")\n",
        "        axs[2].imshow(mask)  #(1024, 1024)\n",
        "        axs[2].axis(\"off\")\n",
        "        axs[3].imshow(prediction) #(1024, 1024)\n",
        "        axs[3].axis(\"off\")\n",
        "\n",
        "\n",
        "        axs[0].set_title(\"image 1\")\n",
        "        axs[1].set_title(\"image 2\")\n",
        "        axs[2].set_title(\"mask\")\n",
        "        axs[3].set_title(\"prediction\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def training_step(self, *args, **kwargs):\n",
        "\n",
        "        batch = args[0]\n",
        "        batch_idx = args[1]\n",
        "\n",
        "        x = batch[\"image\"]\n",
        "        y = batch[\"mask\"]\n",
        "\n",
        "        y_hat = self.forward(x)\n",
        "        y_hat_hard = y_hat.argmax(dim=1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False)\n",
        "        self.train_metrics(y_hat_hard , y)\n",
        "\n",
        "        if batch_idx < 5:\n",
        "            batch[\"prediction\"] = y_hat_hard\n",
        "\n",
        "            for key in [\"image\", \"mask\", \"prediction\"]:\n",
        "                batch[key] = batch[key].cpu()\n",
        "\n",
        "            sample = unbind_samples(batch)[0]\n",
        "\n",
        "            fig = self.plot(sample)\n",
        "\n",
        "            summary_writer = self.logger.experiment\n",
        "            summary_writer.add_figure(f\"image/train/{batch_idx}\", fig, global_step = self.global_step)\n",
        "\n",
        "            plt.close()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, *args, **kwargs):\n",
        "\n",
        "\n",
        "\n",
        "        batch = args[0]\n",
        "        batch_idx = args[1]\n",
        "\n",
        "        x = batch[\"image\"]\n",
        "        y = batch[\"mask\"]\n",
        "\n",
        "        y_hat = self.forward(x)\n",
        "        y_hat_hard = y_hat.argmax(dim=1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "\n",
        "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.val_metrics(y_hat_hard, y)\n",
        "\n",
        "\n",
        "        if batch_idx < 5:\n",
        "            batch[\"prediction\"] = y_hat_hard\n",
        "            for key in [\"image\", \"mask\", \"prediction\"]:\n",
        "                batch[key] = batch[key].cpu()\n",
        "\n",
        "            sample = unbind_samples(batch)[0]\n",
        "            fig = self.plot(sample)\n",
        "            summary_writer = self.logger.experiment\n",
        "            summary_writer.add_figure(f\"image/val/{batch_idx}\", fig, global_step = self.global_step)\n",
        "            plt.close()\n",
        "\n",
        "    def test_step(self, *args, **kwargs): #NEW from original\n",
        "\n",
        "        batch = args[0]\n",
        "        x = batch[\"image\"]\n",
        "        y = batch[\"mask\"]\n",
        "        y_hat = self(x)\n",
        "        y_hat_hard = y_hat.argmax(dim=1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "\n",
        "\n",
        "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.test_metrics(y_hat_hard, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "metadata": {
        "id": "VW4NNolCgTpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mosaic(ref_image, target_image, mask):\n",
        "    # Randomly crop patches from reference and target images\n",
        "    ref_patch = ref_image[np.random.randint(0, ref_image.shape[0]-mask),\n",
        "                          np.random.randint(0, ref_image.shape[1]-mask), :]\n",
        "    target_patch = target_image[np.random.randint(0, target_image.shape[0]-mask),\n",
        "                                np.random.randint(0, target_image.shape[1]-mask), :]\n",
        "\n",
        "    # Create mosaic image by alternating patches\n",
        "    mosaic = np.zeros((mask*2, mask*2, 3), dtype=np.uint8)\n",
        "    mosaic[0:mask, 0:mask, :] = ref_patch\n",
        "    mosaic[0:mask, mask:, :] = target_patch\n",
        "    mosaic[mask:, 0:mask, :] = target_patch\n",
        "    mosaic[mask:, mask:, :] = ref_patch\n",
        "\n",
        "    return mosaic"
      ],
      "metadata": {
        "id": "5T_OjgPbqRw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class boundingbox:\n",
        "        def __init__(self, dataset_dir, batch_size=4, num_epochs=10):\n",
        "            self.dataset_dir = dataset_dir\n",
        "            self.batch_size = batch_size\n",
        "            self.num_epochs = num_epochs\n",
        "\n",
        "\n",
        "            # Define the device to use for training and inference (CPU or GPU)\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "            # Create a dataset object using PyTorch's ImageFolder class\n",
        "            self.train_dataset = torchvision.datasets.ImageFolder(os.path.join(self.dataset_dir, 'train'), transform=self.transform)\n",
        "            self.test_dataset = torchvision.datasets.ImageFolder(os.path.join(self.dataset_dir, 'test'), transform=self.transform)\n",
        "\n",
        "            # Create a data loader for the training and testing datasets\n",
        "            self.train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "            self.test_loader = torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "            # Define the model to use for object detection (e.g., Faster R-CNN)\n",
        "            self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "            num_classes = 4  # A, B, mask, background\n",
        "            in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "            self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "            # Define the optimizer and loss function\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=3, gamma=0.1)\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        def train(self):\n",
        "            # Train the model\n",
        "            for epoch in range(self.num_epochs):\n",
        "                # Set the model to training mode\n",
        "                self.model.train()\n",
        "\n",
        "                # Loop over the training data loader\n",
        "                for i, (images, targets) in enumerate(self.train_loader):\n",
        "                    # Move the data to the device\n",
        "                    images = list(image.to(self.device) for image in images)\n",
        "                    targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                    # Zero out the gradients and compute the forward pass\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss_dict = self.model(images, targets)\n",
        "\n",
        "                    # Compute the total loss\n",
        "                    loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                    # Backpropagate the gradients and update the weights\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    # Print the loss every 10 batches\n",
        "                    if i % 10 == 0:\n",
        "                        print(f'Epoch {epoch}, Batch {i}: {loss.item()}')\n",
        "\n",
        "                # Update the learning rate scheduler\n",
        "                self.lr_scheduler.step()\n",
        "\n",
        "                # Evaluate the model on the test set every epoch\n",
        "                self.model.eval()\n",
        "                test_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for images, targets in self.test_loader:\n",
        "                        # Move the data to the device\n",
        "                        images = list(image.to(self.device) for image in images)\n",
        "                    targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                    # Compute the forward pass\n",
        "                    outputs = self.model(images)\n",
        "\n",
        "                    # Compute the loss\n",
        "                    loss_dict = outputs['losses']\n",
        "                    loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                    # Update the test loss and accuracy\n",
        "                    test_loss += loss.item() * images[0].size(0)\n",
        "                    _, predicted = torch.max(outputs['labels'], 1)\n",
        "                    total += targets[0]['labels'].size(0)\n",
        "                    correct += (predicted == targets[0]['labels']).sum().item()\n",
        "\n",
        "            def predict(self, image_paths):\n",
        "                # Set the model to evaluation mode\n",
        "                self.model.eval()\n",
        "\n",
        "                # Load the images\n",
        "                images = [self.transform(Image.open(path)) for path in image_paths]\n",
        "                images = list(image.to(self.device) for image in images)\n",
        "\n",
        "                # Compute the forward pass\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(images)\n",
        "\n",
        "                # Convert the outputs to bounding boxes\n",
        "                bboxes = []\n",
        "                for output in outputs:\n",
        "                    bbox = output['boxes'].cpu().numpy()[0]\n",
        "                    bboxes.append(bbox)\n",
        "\n",
        "                return bboxes\n",
        "\n",
        "def bounding_box(img1, img2, mask):\n",
        "    try:\n",
        "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    except:\n",
        "        _, contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    boxes = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        boxes.append((x, y, w, h))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
        "    ax[0].imshow(img1)\n",
        "    ax[0].set_title('Image 1')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(img2)\n",
        "    ax[1].set_title('Image 2')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    ax[2].imshow(mask)\n",
        "    ax[2].set_title('Image 3')\n",
        "    ax[2].axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QPEh1AfmGoWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kornia.augmentation import AugmentationSequential"
      ],
      "metadata": {
        "id": "fd8wDV8vsFul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afG8wnlZUQ1A"
      },
      "outputs": [],
      "source": [
        "#Dataset\n",
        "class LEVIRCDPlusDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_size=12,\n",
        "        num_workers=0,\n",
        "        val_split_pct=0.2,\n",
        "        patch_size=(256,256),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.val_split_pct = val_split_pct\n",
        "        self.patch_size = patch_size\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def on_after_batch_transfer(self, batch, batch_idx):\n",
        "        if (\n",
        "            hasattr(self, \"trainer\")\n",
        "            and self.trainer is not None\n",
        "            and hasattr(self.trainer, \"training\")\n",
        "            and self.trainer.training\n",
        "        ):\n",
        "\n",
        "            x = batch[\"image\"]  #[12, 6, 1024, 1024]\n",
        "            y = batch[\"mask\"].float().unsqueeze(1)  #[12, 1024, 1024] --> [12, 1, 1024, 1024])\n",
        "\n",
        "            train_augmentations = K.AugmentationSequential(\n",
        "                K.RandomRotation(p=0.5, degrees=90),\n",
        "                K.RandomHorizontalFlip(p=0.5),\n",
        "                K.RandomVerticalFlip(p=0.5),\n",
        "                K.RandomCrop(self.patch_size),\n",
        "                K.RandomSharpness(p=0.5),\n",
        "                data_keys=[\"input\", \"mask\"],\n",
        "            )\n",
        "\n",
        "            x, y = train_augmentations(x, y)\n",
        "\n",
        "\n",
        "            batch[\"image\"] = x\n",
        "            batch[\"mask\"] = y.squeeze(1).long()\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def preprocess(self, sample):\n",
        "\n",
        "        sample[\"image\"] = (sample[\"image\"]/255.0).float() #[2, 3, 1024, 1024]\n",
        "        sample[\"image\"] = torch.flatten(sample[\"image\"], 0, 1) #[6, 1024, 1024]\n",
        "        sample[\"mask\"] = sample[\"mask\"].long() #[1024, 1024]\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        train_transforms = Compose ([self.preprocess])\n",
        "        test_transforms = Compose ([self.preprocess])\n",
        "\n",
        "        train_dataset = LEVIRCDPlus(\n",
        "            split=\"train\", transforms=train_transforms, **self.kwargs\n",
        "        )\n",
        "\n",
        "        if self.val_split_pct > 0.0:\n",
        "\n",
        "            self.train_dataset, self.val_dataset, _ = dataset_split(\n",
        "                train_dataset, val_pct=self.val_split_pct, test_pct=0.0\n",
        "            )\n",
        "        else:\n",
        "            self.train_dataset = train_dataset\n",
        "            self.val_dataset = train_dataset\n",
        "\n",
        "        self.test_dataset = LEVIRCDPlus(\n",
        "            split=\"test\", transforms=test_transforms, **self.kwargs\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=True,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpVJPzqUknom"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "\n",
        "import keras.backend as KB\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "    intersection = KB.sum(y_true * y_pred, axis=[1,2,3])\n",
        "    union = KB.sum(y_true, axis=[1,2,3]) + KB.sum(y_pred, axis=[1,2,3])\n",
        "    return KB.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
        "def dice_p_bce(in_gt, in_pred):\n",
        "    return 0.05*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
        "def true_positive_rate(y_true, y_pred):\n",
        "    return KB.sum(KB.flatten(y_true)*KB.flatten(KB.round(y_pred)))/KB.sum(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcrWS4IQUQ1B"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "datamodule = LEVIRCDPlusDataModule( root = \"LEVIRCDPlus\", batch_size = batch_size, num_workers = num_workers, val_split_pct = val_split_pct, patch_size = (patch_size, patch_size),)\n",
        "task = CustomSemanticSegmentationTask(model=\"unet\", backbone=\"resnet18\", weights=\"imagenet\", in_channels=6, num_classes=2, loss=\"ce\", ignore_index=None, learning_rate=lr, learning_rate_schedule_patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLokA6x-UQ1B"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"val_loss\",\n",
        "    dirpath=exp_dir,\n",
        "    save_top_k=1,\n",
        "    save_last=True,\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.00,\n",
        "    patience=10,\n",
        ")\n",
        "\n",
        "\n",
        "tb_logger = TensorBoardLogger(\n",
        "    save_dir=\"logs/\",\n",
        "    name=exp_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img1 = cv2.imread('/content/LEVIRCDPlus/LEVIR-CD+/test/A/train_638.png')\n",
        "img2 = cv2.imread('/content/LEVIRCDPlus/LEVIR-CD+/test/B/train_638.png')\n",
        "mask = cv2.imread('/content/LEVIRCDPlus/LEVIR-CD+/test/label/train_638.png',0)\n",
        "bounding_box(img1, img2, mask)"
      ],
      "metadata": {
        "id": "l0x1NtvUKcW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvyDW66fUQ1C"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        "    logger=[tb_logger],\n",
        "    default_root_dir=exp_dir,\n",
        "    min_epochs=1,\n",
        "    max_epochs=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install basenet"
      ],
      "metadata": {
        "id": "gAHAGneHTSJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylocron"
      ],
      "metadata": {
        "id": "otu3YWlXi9S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mish, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "ACTIVATIONS = {\n",
        "    'mish': Mish(),\n",
        "    'linear': nn.Identity()\n",
        "}\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, activation='mish'):\n",
        "        super(Conv, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size//2, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            ACTIVATIONS[activation]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class CSPBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, residual_activation='linear'):\n",
        "        super(CSPBlock, self).__init__()\n",
        "\n",
        "        if hidden_channels is None:\n",
        "            hidden_channels = out_channels\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            Conv(in_channels, hidden_channels, 1),\n",
        "            Conv(hidden_channels, out_channels, 3)\n",
        "        )\n",
        "\n",
        "        self.activation = ACTIVATIONS[residual_activation]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(x+self.block(x))\n",
        "\n",
        "class CSPFirstStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CSPFirstStage, self).__init__()\n",
        "\n",
        "        self.downsample_conv = Conv(in_channels, out_channels, 3, stride=2)\n",
        "\n",
        "        self.split_conv0 = Conv(out_channels, out_channels, 1)\n",
        "        self.split_conv1 = Conv(out_channels, out_channels, 1)\n",
        "\n",
        "        self.blocks_conv = nn.Sequential(\n",
        "            CSPBlock(out_channels, out_channels, in_channels),\n",
        "            Conv(out_channels, out_channels, 1)\n",
        "        )\n",
        "\n",
        "        self.concat_conv = Conv(out_channels*2, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample_conv(x)\n",
        "\n",
        "        x0 = self.split_conv0(x)\n",
        "        x1 = self.split_conv1(x)\n",
        "\n",
        "        x1 = self.blocks_conv(x1)\n",
        "\n",
        "        x = torch.cat([x0, x1], dim=1)\n",
        "        x = self.concat_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CSPStage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_blocks):\n",
        "        super(CSPStage, self).__init__()\n",
        "\n",
        "        self.downsample_conv = Conv(in_channels, out_channels, 3, stride=2)\n",
        "\n",
        "        self.split_conv0 = Conv(out_channels, out_channels//2, 1)\n",
        "        self.split_conv1 = Conv(out_channels, out_channels//2, 1)\n",
        "\n",
        "        self.blocks_conv = nn.Sequential(\n",
        "            *[CSPBlock(out_channels//2, out_channels//2) for _ in range(num_blocks)],\n",
        "            Conv(out_channels//2, out_channels//2, 1)\n",
        "        )\n",
        "\n",
        "        self.concat_conv = Conv(out_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.downsample_conv(x)\n",
        "\n",
        "        x0 = self.split_conv0(x)\n",
        "        x1 = self.split_conv1(x)\n",
        "\n",
        "        x1 = self.blocks_conv(x1)\n",
        "\n",
        "        x = torch.cat([x0, x1], dim=1)\n",
        "        x = self.concat_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CSPDarknet53(nn.Module):\n",
        "    def __init__(self, stem_channels=32, feature_channels=[64, 128, 256, 512, 1024], num_features=1):\n",
        "        super(CSPDarknet53, self).__init__()\n",
        "\n",
        "        self.stem_conv = Conv(3, stem_channels, 3)\n",
        "\n",
        "        self.stages = nn.ModuleList([\n",
        "            CSPFirstStage(stem_channels, feature_channels[0]),\n",
        "            CSPStage(feature_channels[0], feature_channels[1], 2),\n",
        "            CSPStage(feature_channels[1], feature_channels[2], 8),\n",
        "            CSPStage(feature_channels[2], feature_channels[3], 8),\n",
        "            CSPStage(feature_channels[3], feature_channels[4], 4)\n",
        "        ])\n",
        "\n",
        "        self.feature_channels = feature_channels\n",
        "        self.num_features = num_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem_conv(x)\n",
        "\n",
        "        features = []\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "            features.append(x)\n",
        "\n",
        "        return features[-self.num_features:]\n",
        "\n",
        "def _BuildCSPDarknet53(num_features=3):\n",
        "    model = CSPDarknet53(num_features=num_features)\n",
        "\n",
        "    return model, model.feature_channels[-num_features:]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = CSPDarknet53()\n",
        "    x = torch.randn(1, 3, 224, 224)\n",
        "    y = model(x)"
      ],
      "metadata": {
        "id": "o_T2PN-tiTru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as resnet50"
      ],
      "metadata": {
        "id": "WqIQXxbzkjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outS(i):\n",
        "    i = int(i)\n",
        "    i = (i + 1) / 2\n",
        "    i = int(np.ceil((i + 1) / 2.0))\n",
        "    i = (i + 1) / 2\n",
        "    return i\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"3x3 convolution with padding\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)"
      ],
      "metadata": {
        "id": "AU33zJLToYgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import basenet\n",
        "import torch.nn as nn\n",
        "from torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n",
        "    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch_ver = torch.__version__[:3]\n",
        "\n",
        "\n",
        "class PCAM_Module(Module):\n",
        "    \"\"\" Position attention module\"\"\"\n",
        "    def __init__(self, in_dim):\n",
        "        super(PCAM_Module, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "\n",
        "        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "        self.gamma = Parameter(torch.zeros(1))\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            inputs :\n",
        "                x : input feature maps( B X C X H X W)\n",
        "            returns :\n",
        "                out : attention value + input feature\n",
        "                attention: B X (HxW) X (HxW)\n",
        "        \"\"\"\n",
        "        create_mosaic(img1, img2, mask)\n",
        "        m_batchsize, C, height, width = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(m_batchsize, C, height, width)\n",
        "\n",
        "        out = self.gamma*out + x\n",
        "        return out\n",
        "\n",
        "\n",
        "class CCAM_Module(Module):\n",
        "    \"\"\" Channel attention module\"\"\"\n",
        "    def __init__(self, in_dim):\n",
        "        super(CCAM_Module, self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "\n",
        "\n",
        "        self.gamma = Parameter(torch.zeros(1))\n",
        "        self.softmax  = Softmax(dim=-1)\n",
        "\n",
        "class ChangeDetectionModel(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(ChangeDetectionModel, self).__init__()\n",
        "\n",
        "        # Load the CSPDarknet-53 backbone\n",
        "        self.backbone = CSPDarknet53([1, 2, 8, 8, 4], num_classes=in_channels)\n",
        "\n",
        "        # Change detection heads\n",
        "        self.detection_head_s = CCAM_Module(in_channels, num_classes)\n",
        "        self.detection_head_m = CCAM_Module(in_channels, num_classes)\n",
        "        self.detection_head_l = CCAM_Module(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get features from the shared backbone model\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Get features from the PAFPN model\n",
        "        features_s, features_m, features_l = resnet50(features)\n",
        "\n",
        "        # Get change detection predictions from each detection head\n",
        "        change_detection_s = self.detection_head_s(features_s)\n",
        "        change_detection_m = self.detection_head_m(features_m)\n",
        "        change_detection_l = self.detection_head_l(features_l)\n",
        "\n",
        "        # Upsample change detection predictions to the input size\n",
        "        change_detection_s = nn.functional.interpolate(change_detection_s, size=x.size()[2:], mode='bilinear', align_corners=True)\n",
        "        change_detection_m = nn.functional.interpolate(change_detection_m, size=x.size()[2:], mode='bilinear', align_corners=True)\n",
        "        change_detection_l = nn.functional.interpolate(change_detection_l, size=x.size()[2:], mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Combine change detection predictions from all heads\n",
        "        change_detection = change_detection_s + change_detection_m + change_detection_l\n",
        "\n",
        "        return change_detection\n",
        "\n",
        "class DCA_det(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, norm_layer):\n",
        "        super(DCA_det, self).__init__()\n",
        "        inter_channels = in_channels // 4\n",
        "        self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "                                    norm_layer(inter_channels),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "                                    norm_layer(inter_channels),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        self.sa = PCAM_Module(inter_channels)\n",
        "        self.sc = CCAM_Module(inter_channels)\n",
        "        self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
        "                                    norm_layer(inter_channels),\n",
        "                                    nn.ReLU())\n",
        "        self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
        "                                    norm_layer(inter_channels),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        self.conv6 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1))\n",
        "        self.conv7 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1))\n",
        "\n",
        "        self.conv8 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat1 = self.conv5a(x)\n",
        "        sa_feat = self.sa(feat1)\n",
        "        sa_conv = self.conv51(sa_feat)\n",
        "        sa_output = self.conv6(sa_conv)\n",
        "        feat2 = self.conv5c(x)\n",
        "        sc_feat = self.sc(feat2)\n",
        "        sc_conv = self.conv52(sc_feat)\n",
        "        sc_output = self.conv7(sc_conv)\n",
        "\n",
        "        feat_sum = sa_conv + sc_conv\n",
        "\n",
        "        sasc_output = self.conv8(feat_sum)\n",
        "\n",
        "        return sa_output,sc_output,sasc_output\n",
        "\n",
        "\n",
        "class DCAM(nn.Module):\n",
        "    #expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(DCAM, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, affine=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, affine=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self,norm_flag = 'l2'):\n",
        "        super(UNet, self).__init__()\n",
        "        self.CNN = model()\n",
        "        if norm_flag == 'l2':\n",
        "           self.norm = F.normalize\n",
        "        if norm_flag == 'exp':\n",
        "            self.norm = nn.Softmax2d()\n",
        "    '''''''''\n",
        "    def forward(self,t0,t1):\n",
        "        out_t0_embedding = self.CNN(t0)\n",
        "        out_t1_embedding = self.CNN(t1)\n",
        "        #out_t0_conv5_norm,out_t1_conv5_norm = self.norm(out_t0_conv5),self.norm(out_t1_conv5)\n",
        "        #out_t0_fc7_norm,out_t1_fc7_norm = self.norm(out_t0_fc7),self.norm(out_t1_fc7)\n",
        "        out_t0_embedding_norm,out_t1_embedding_norm = self.norm(out_t0_embedding),self.norm(out_t1_embedding)\n",
        "        return [out_t0_embedding_norm,out_t1_embedding_norm]\n",
        "    '''''''''\n",
        "\n",
        "    def forward(self,t0,t1):\n",
        "\n",
        "        out_t0_conv5,out_t0_fc7,out_t0_embedding = self.CNN(t0)\n",
        "        out_t1_conv5,out_t1_fc7,out_t1_embedding = self.CNN(t1)\n",
        "        out_t0_conv5_norm,out_t1_conv5_norm = self.norm(out_t0_conv5,2,dim=1),self.norm(out_t1_conv5,2,dim=1)\n",
        "        out_t0_fc7_norm,out_t1_fc7_norm = self.norm(out_t0_fc7,2,dim=1),self.norm(out_t1_fc7,2,dim=1)\n",
        "        out_t0_embedding_norm,out_t1_embedding_norm = self.norm(out_t0_embedding,2,dim=1),self.norm(out_t1_embedding,2,dim=1)\n",
        "        return [out_t0_conv5_norm,out_t1_conv5_norm],[out_t0_fc7_norm,out_t1_fc7_norm],[out_t0_embedding_norm,out_t1_embedding_norm]"
      ],
      "metadata": {
        "id": "qWffqkWkSthb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-uO7WWpUQ1C"
      },
      "outputs": [],
      "source": [
        "#torch.set_float32_matmul_precision('High')\n",
        "_ = trainer.fit(model=task, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw9FCr9qUQ1C"
      },
      "outputs": [],
      "source": [
        "trainer.test(model=task, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path2 = '/content/my_model.ckpt'\n",
        "my_model = torch.load(path2)"
      ],
      "metadata": {
        "id": "PhJcGUIiMQBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir='/content/logs/exp_1/version_0'"
      ],
      "metadata": {
        "id": "b1NkDUJ2PDC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/logs/exp_1/version_0/events.out.tfevents.1682142165.c6e04740d553.2251.0')"
      ],
      "metadata": {
        "id": "cyKoDdzURIV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f3f71acbe028193bc0bf25c7bbd98f54ca429f192d17b6451f61294f7eafb542"
      }
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}